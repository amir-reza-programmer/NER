{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:04:46.792094Z",
     "iopub.status.busy": "2024-12-21T20:04:46.791868Z",
     "iopub.status.idle": "2024-12-21T20:04:55.696740Z",
     "shell.execute_reply": "2024-12-21T20:04:55.696055Z",
     "shell.execute_reply.started": "2024-12-21T20:04:46.792060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320c94847460484ea92db70611eb294b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f458d2efb52424f88c7fde69384640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59836926b1824c5eb9fb21926eba8c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/431k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec55c27131e74dc5af876e916e8640d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e379823a3a44dead879a55336125ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac73507caa504e27bc45b88174808436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12763c602e34f519273edc727ad3424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27c126cd44a4b4bb4f9b113ae6abaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b9fa2d30794978a207a052a18128af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c4edd7d28e43a7908da111a69a4fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1128fd273c5c45e987d31517fc1ba4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb5c29cf6564b49bf7d47de8e0e0be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d69a01a5244a8590ebaca60e4c095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "persian_dataset = load_dataset('AliFartout/PEYMA-ARMAN-Mixed')\n",
    "english_dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:05:18.605553Z",
     "iopub.status.busy": "2024-12-21T20:05:18.605239Z",
     "iopub.status.idle": "2024-12-21T20:05:18.609799Z",
     "shell.execute_reply": "2024-12-21T20:05:18.608874Z",
     "shell.execute_reply.started": "2024-12-21T20:05:18.605525Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_ids = {'O': 0, 'B_PER': 1, 'I_PER': 2, 'B_ORG': 3, 'I_ORG': 4, 'B_LOC': 5, 'I_LOC': 6, 'B_MISC': 7, 'I_MISC': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'style=\"font-family: Vazir; width: 85%; font-size: 18px;\">دو تا دیتاست از لحاظ خروجی با هم فرق دارند. دیتاست فارسی علاوه بر اینکه انواع موجودیت های بیشتری را تشخیص میدهد آیدی که برای تگ ها در نظر میگیرد با دیتاست انگلیسی متفاوت است. دیتاست که برای آموزش مدل استفاده میشود باید یکپارچه باشد بنابراین این یکسان سازی دو دیتاست و سپس ترکیب آن ها را در ادامه انجام میدهیم.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:05:20.615291Z",
     "iopub.status.busy": "2024-12-21T20:05:20.614620Z",
     "iopub.status.idle": "2024-12-21T20:05:20.619696Z",
     "shell.execute_reply": "2024-12-21T20:05:20.618700Z",
     "shell.execute_reply.started": "2024-12-21T20:05:20.615256Z"
    }
   },
   "outputs": [],
   "source": [
    "def modify_label(sample):\n",
    "    ner_tags = list(map(lambda x: tag_ids.get(x, 7 if x[0] == 'B' else 8), sample['ner_tags_names']))\n",
    "    sample['ner_tags'] = ner_tags\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:05:23.907381Z",
     "iopub.status.busy": "2024-12-21T20:05:23.906634Z",
     "iopub.status.idle": "2024-12-21T20:05:26.724468Z",
     "shell.execute_reply": "2024-12-21T20:05:26.723684Z",
     "shell.execute_reply.started": "2024-12-21T20:05:23.907349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a429191a01bf49b4a81ba4fc929f8e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe7baf31d4b4571a4ea76767deb91d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca32fe1428b4becb2c293c44ce93506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_persian_dataset = persian_dataset.map(modify_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:05:28.905088Z",
     "iopub.status.busy": "2024-12-21T20:05:28.904755Z",
     "iopub.status.idle": "2024-12-21T20:05:28.911642Z",
     "shell.execute_reply": "2024-12-21T20:05:28.910661Z",
     "shell.execute_reply.started": "2024-12-21T20:05:28.905057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['یوکوویچ',\n",
       "  'متولد',\n",
       "  'دانمارک',\n",
       "  'است',\n",
       "  'اما',\n",
       "  'والدین',\n",
       "  'او',\n",
       "  'صرب',\n",
       "  'هستند',\n",
       "  '.'],\n",
       " 'ner_tags': [1, 0, 5, 0, 0, 0, 0, 5, 0, 0],\n",
       " 'ner_tags_names': ['B_PER',\n",
       "  'O',\n",
       "  'B_LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B_LOC',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_persian_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T21:27:16.226680Z",
     "iopub.status.busy": "2024-12-21T21:27:16.225855Z",
     "iopub.status.idle": "2024-12-21T21:27:16.235969Z",
     "shell.execute_reply": "2024-12-21T21:27:16.235158Z",
     "shell.execute_reply.started": "2024-12-21T21:27:16.226642Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class XLMRobertaForTokenClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        logits = self.classifier(self.dropout(last_hidden_state))  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {'loss': loss, 'logits': logits} if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:03.716490Z",
     "iopub.status.busy": "2024-12-21T20:06:03.715299Z",
     "iopub.status.idle": "2024-12-21T20:06:03.722711Z",
     "shell.execute_reply": "2024-12-21T20:06:03.721767Z",
     "shell.execute_reply.started": "2024-12-21T20:06:03.716441Z"
    }
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            aligned_labels.append(-100)  # Special token or padding\n",
    "        elif word_id != previous_word_id:\n",
    "            aligned_labels.append(labels[word_id])  # Assign label to the first subword\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Ignore subsequent subwords\n",
    "        previous_word_id = word_id\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:07.066817Z",
     "iopub.status.busy": "2024-12-21T20:06:07.066497Z",
     "iopub.status.idle": "2024-12-21T20:06:07.081236Z",
     "shell.execute_reply": "2024-12-21T20:06:07.080571Z",
     "shell.execute_reply.started": "2024-12-21T20:06:07.066789Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset = english_dataset.remove_columns(['id','pos_tags', 'chunk_tags'])\n",
    "updated_persian_dataset = updated_persian_dataset.remove_columns(['ner_tags_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:09.783565Z",
     "iopub.status.busy": "2024-12-21T20:06:09.783217Z",
     "iopub.status.idle": "2024-12-21T20:06:09.790478Z",
     "shell.execute_reply": "2024-12-21T20:06:09.789651Z",
     "shell.execute_reply.started": "2024-12-21T20:06:09.783535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tokens': ['EU',\n",
       "   'rejects',\n",
       "   'German',\n",
       "   'call',\n",
       "   'to',\n",
       "   'boycott',\n",
       "   'British',\n",
       "   'lamb',\n",
       "   '.'],\n",
       "  'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]},\n",
       " {'tokens': ['یوکوویچ',\n",
       "   'متولد',\n",
       "   'دانمارک',\n",
       "   'است',\n",
       "   'اما',\n",
       "   'والدین',\n",
       "   'او',\n",
       "   'صرب',\n",
       "   'هستند',\n",
       "   '.'],\n",
       "  'ner_tags': [1, 0, 5, 0, 0, 0, 0, 5, 0, 0]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset['train'][0], updated_persian_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:29:50.080920Z",
     "iopub.status.busy": "2024-12-20T11:29:50.080572Z",
     "iopub.status.idle": "2024-12-20T11:29:50.087528Z",
     "shell.execute_reply": "2024-12-20T11:29:50.086575Z",
     "shell.execute_reply.started": "2024-12-20T11:29:50.080890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:32.473173Z",
     "iopub.status.busy": "2024-12-21T20:06:32.472730Z",
     "iopub.status.idle": "2024-12-21T20:06:35.359628Z",
     "shell.execute_reply": "2024-12-21T20:06:35.358689Z",
     "shell.execute_reply.started": "2024-12-21T20:06:32.473103Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "list1 = [dict(item) for item in english_dataset[\"train\"]]\n",
    "list2 = [dict(item) for item in updated_persian_dataset[\"train\"]]\n",
    "combined_list = list1 + list2\n",
    "data_dict = {key: [d[key] for d in combined_list] for key in combined_list[0].keys()}\n",
    "\n",
    "final_train_dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:38.292064Z",
     "iopub.status.busy": "2024-12-21T20:06:38.291740Z",
     "iopub.status.idle": "2024-12-21T20:06:38.791320Z",
     "shell.execute_reply": "2024-12-21T20:06:38.790434Z",
     "shell.execute_reply.started": "2024-12-21T20:06:38.292034Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "list1 = [dict(item) for item in english_dataset[\"validation\"]]\n",
    "list2 = [dict(item) for item in updated_persian_dataset[\"validation\"]]\n",
    "combined_list = list1 + list2\n",
    "data_dict = {key: [d[key] for d in combined_list] for key in combined_list[0].keys()}\n",
    "\n",
    "final_validation_dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:41.772617Z",
     "iopub.status.busy": "2024-12-21T20:06:41.771978Z",
     "iopub.status.idle": "2024-12-21T20:06:41.777859Z",
     "shell.execute_reply": "2024-12-21T20:06:41.776969Z",
     "shell.execute_reply.started": "2024-12-21T20:06:41.772576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 40425\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:06:49.737061Z",
     "iopub.status.busy": "2024-12-21T20:06:49.736730Z",
     "iopub.status.idle": "2024-12-21T20:06:52.873253Z",
     "shell.execute_reply": "2024-12-21T20:06:52.872192Z",
     "shell.execute_reply.started": "2024-12-21T20:06:49.737030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2028fa9ab4f93a3e75aa5a9f61642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9e92d8dff444189dcb4f0fc471947e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b077d6833b4635b08fedd330ab4fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b109323f453e40dc9916c684923fbd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "    aligned_labels = align_labels_with_tokens(examples[\"ner_tags\"], word_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"font-family: Vazir; width: 85%; font-size: 18px;\">برای اینکه توکنایزر ممکن است بعضی کلمات را در هنگام توکنایز کردن به subword تبدیل کند در این صورت دیگر خروجی مدل که یک لیست از تگ های موجودیت ها (به ازای هر کلمه) است دیگر هماهنگی نخواهند داشت برای همین باید این شرایط هندل کنیم.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:08:14.545974Z",
     "iopub.status.busy": "2024-12-21T20:08:14.545139Z",
     "iopub.status.idle": "2024-12-21T20:08:14.551424Z",
     "shell.execute_reply": "2024-12-21T20:08:14.550527Z",
     "shell.execute_reply.started": "2024-12-21T20:08:14.545937Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:42:52.428637Z",
     "iopub.status.busy": "2024-12-20T11:42:52.427857Z",
     "iopub.status.idle": "2024-12-20T11:42:52.435156Z",
     "shell.execute_reply": "2024-12-20T11:42:52.434350Z",
     "shell.execute_reply.started": "2024-12-20T11:42:52.428602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T21:16:35.409644Z",
     "iopub.status.busy": "2024-12-21T21:16:35.408778Z",
     "iopub.status.idle": "2024-12-21T21:16:35.416827Z",
     "shell.execute_reply": "2024-12-21T21:16:35.416042Z",
     "shell.execute_reply.started": "2024-12-21T21:16:35.409605Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    lengths = [len(sentence) for sentence in sentences]\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=0)  # 0 for <pad>\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 for ignore_index\n",
    "    attention_mask = (padded_sentences != 0).long()  # Mask is 1 for non-padding tokens, 0 otherwise\n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_sentences),\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': torch.tensor(padded_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:08:19.547125Z",
     "iopub.status.busy": "2024-12-21T20:08:19.546777Z",
     "iopub.status.idle": "2024-12-21T20:08:27.352375Z",
     "shell.execute_reply": "2024-12-21T20:08:27.351491Z",
     "shell.execute_reply.started": "2024-12-21T20:08:19.547080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e53474e73c43529ef73ee72dcc4ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9460a0887850439cbdbf30544d5991c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = final_train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_validation_dataset = final_validation_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T20:24:21.198892Z",
     "iopub.status.busy": "2024-12-21T20:24:21.198008Z",
     "iopub.status.idle": "2024-12-21T20:24:21.206211Z",
     "shell.execute_reply": "2024-12-21T20:24:21.205306Z",
     "shell.execute_reply.started": "2024-12-21T20:24:21.198854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [0,\n",
       "  3747,\n",
       "  456,\n",
       "  75161,\n",
       "  7,\n",
       "  30839,\n",
       "  11782,\n",
       "  47,\n",
       "  25299,\n",
       "  47924,\n",
       "  18,\n",
       "  56101,\n",
       "  21,\n",
       "  6492,\n",
       "  6,\n",
       "  5,\n",
       "  2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100,\n",
       "  3,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  7,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T21:27:23.990457Z",
     "iopub.status.busy": "2024-12-21T21:27:23.990094Z",
     "iopub.status.idle": "2024-12-21T21:41:26.319056Z",
     "shell.execute_reply": "2024-12-21T21:41:26.318218Z",
     "shell.execute_reply.started": "2024-12-21T21:27:23.990423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 40425\n",
      "})\n",
      "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\n",
      "Value(dtype='int64', id=None)\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2710297511.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/tmp/ipykernel_23/435131875.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(padded_sentences),\n",
      "/tmp/ipykernel_23/435131875.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(padded_labels),\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2408, 'grad_norm': 2.1845710277557373, 'learning_rate': 2.0344009489916967e-05, 'epoch': 0.5931198102016607}\n",
      "{'eval_loss': 0.1031973585486412, 'eval_runtime': 36.2539, 'eval_samples_per_second': 180.56, 'eval_steps_per_second': 3.779, 'epoch': 0.5931198102016607}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/435131875.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(padded_sentences),\n",
      "/tmp/ipykernel_23/435131875.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(padded_labels),\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 804.3589, 'train_samples_per_second': 50.257, 'train_steps_per_second': 1.048, 'train_loss': 0.17864773253670507, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/435131875.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(padded_sentences),\n",
      "/tmp/ipykernel_23/435131875.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(padded_labels),\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08001686632633209, 'eval_runtime': 36.4243, 'eval_samples_per_second': 179.715, 'eval_steps_per_second': 3.761, 'epoch': 1.0}\n",
      "Evaluation Results: {'eval_loss': 0.08001686632633209, 'eval_runtime': 36.4243, 'eval_samples_per_second': 179.715, 'eval_steps_per_second': 3.761, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Roberta-fa-en-ner\",\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=False,\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "print(tokenized_train_dataset)\n",
    "print(tokenized_train_dataset.features[\"ner_tags\"])\n",
    "print(tokenized_train_dataset.features[\"ner_tags\"].feature)\n",
    "all_labels = set(tag for tags in tokenized_train_dataset[\"ner_tags\"] for tag in tags)\n",
    "print(all_labels)\n",
    "num_labels = len(all_labels)\n",
    "model = XLMRobertaForTokenClassification(model_name, num_labels=num_labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
